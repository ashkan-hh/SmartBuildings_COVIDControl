{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed85e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import import_ipynb\n",
    "\n",
    "# import ipdb\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tfc\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# from tensorflow.keras import optimizers\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Activation\n",
    "# import tensorflow.keras.backend as K\n",
    "# import tensorflow.compat.v1.keras.backend as Kc\n",
    "\n",
    "# from Environment import Simple1Room\n",
    "# from Environment import offline_sim\n",
    "\n",
    "\n",
    "\n",
    "class Stochastic_AC_PG:\n",
    "    \"\"\"\n",
    "    a class for actor-critic stochastic policy-gradient RL\n",
    "    \n",
    "    a Stochastic_AC_PG object is used to implement an actor-critic \n",
    "    stochastic policy-gradient RL with eligibility traces that can \n",
    "    handle both types of average rewards: average reward per time step &\n",
    "    average reward per step\n",
    "    \n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    Td: float\n",
    "        the user-defined desired temperature\n",
    "    learning_rate_policy_mu: float\n",
    "        learning rate for policy mean. It should take a value in [0,1]\n",
    "        (default 0.01)\n",
    "    learning_rate_policy_sigma: float\n",
    "        learning rate for policy variance. It should take a value in \n",
    "        [0,1] (default 0.01)\n",
    "    learning_rate_valuefunc: float\n",
    "        learning rate for state value function. It should take a value\n",
    "        in [0,1] (default 0.01)\n",
    "    learning_rate_avereward: float\n",
    "        learning rate for average reward. It should take a value in\n",
    "        [0,1] (default 0.01)\n",
    "    elig_decay_policy: float\n",
    "        eligibilit decay parameter for policy. It should take a value in\n",
    "        [0,1] (default 0.2)\n",
    "    elig_decay_valuefunc: float\n",
    "        eligibilit decay parameter for state-value function. It should \n",
    "        take a value in [0,1] (default 0.2)\n",
    "    T_upperbound: float\n",
    "        upper limit of switch-OFF temperature. (default Td+5)\n",
    "    T_lowerbound: float\n",
    "        lower limit of switch-ON temperature. (default Td-5)\n",
    "    PerTimeStep: bool\n",
    "        if set to True average reward per time step is used, otherwise\n",
    "        if set to False average reward per step is used (default True)\n",
    "    reward_decay: float\n",
    "        reward decay parameter. It should take a value in [0,1] \n",
    "        (default 1)\n",
    "    n_features: int\n",
    "        number of features for the feature vector (default 2)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    initialize():\n",
    "        initialized all the system parameters that we want to learn\n",
    "    choose_action(state):\n",
    "        chooses an action (temp. threshold) from a Gaussian distribution\n",
    "    learn(S, A, Sp, R, delta_time):\n",
    "        updates all the system parameters we want to learn\n",
    "    value_func(state):\n",
    "        evaluates the state-value function at a given state\n",
    "    value_func_grad(state):\n",
    "        evaluates the state-value function gradient wrt its parameters \n",
    "        at a given state\n",
    "    policy_func_grad(state, A):\n",
    "        evaluates the policy (its log) gradient wrt its parameters at a \n",
    "        given state and action\n",
    "    feature_vec(cls,state):\n",
    "    this outputs a feature vector\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 l1_upperbound,\n",
    "                 l1_lowerbound,\n",
    "                 theta_mu0 = None,\n",
    "                 learning_rate_policy_mu=0.01,\n",
    "                 learning_rate_policy_sigma=0.01,\n",
    "                 learning_rate_valuefunc=0.01,\n",
    "                 learning_rate_avereward=0.01,\n",
    "                 elig_decay_policy=0.2,\n",
    "                 elig_decay_valuefunc=0.2,\n",
    "                 policy_update_freq=1,\n",
    "                 PerTimeStep=True,\n",
    "                 reward_decay=1,\n",
    "                 n_features=2):\n",
    "\n",
    "        self.l1up = l1_upperbound\n",
    "        self.l1low = l1_lowerbound\n",
    "        self.alpha_theta_mu = learning_rate_policy_mu\n",
    "        self.alpha_theta_sigma = learning_rate_policy_sigma\n",
    "        self.alpha_w = learning_rate_valuefunc\n",
    "        self.eta = learning_rate_avereward\n",
    "        self.gamma = reward_decay\n",
    "        self.lambda_theta = elig_decay_policy\n",
    "        self.lambda_w = elig_decay_valuefunc\n",
    "        self.PerTimeStep = PerTimeStep\n",
    "        self.n_features = n_features\n",
    "        self.policy_update_freq=policy_update_freq\n",
    "        if theta_mu0 is None:\n",
    "            self.theta_mu0 = self.normalize_action_func(1.0)\n",
    "        else:\n",
    "            self.theta_mu0 = self.normalize_action_func(theta_mu0)\n",
    "            \n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        initializes all the parameters.\n",
    "        \n",
    "        initialized values:\n",
    "        theta_sigma = 0.0\n",
    "        theta_mu = [Td-3, Td+3]\n",
    "        w_valuefunc = [np.random.uniform(low=-1, high=1.0, size=None),\n",
    "                       np.random.uniform(low=-1, high=1.0, size=None)]\n",
    "        Rave = 0.0\n",
    "        \n",
    "        all the initial values of eligibility traces are set to 0.0\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initializing parameters of the policy (theta_mu = [theta_ON,\n",
    "        # theta_OFF], theta_sigma) and value function (w=[w_OFF, w_ON]) as well\n",
    "        # as the average reward (Rave)\n",
    "\n",
    "        self.theta_sigma = 0.0\n",
    "#         theta_ON = self.Td - 4.0\n",
    "#         theta_OFF = self.Td + 4.0\n",
    "#         self.theta_mu = self.l1low\n",
    "#         self.theta_mu = self.normalize_action_func(self.l1low)\n",
    "        \n",
    "        self.theta_mu = self.theta_mu0\n",
    "        \n",
    "        self.z_theta_mu = 0.0\n",
    "        self.z_theta_sigma = 0.0\n",
    "        self.z_w = 0.0\n",
    "        self.Rave = 0\n",
    "        \n",
    "#         w_ON = np.random.uniform(low=-1, high=1.0, size=None)\n",
    "#         w_OFF = np.random.uniform(low=-1, high=1.0, size=None)\n",
    "#         self.w_valuefunc = np.array([w_OFF, w_ON])\n",
    "        \n",
    "        self.w_valuefunc = np.random.uniform(low=-1, high=1.0, size=None)\n",
    "        \n",
    "        self.policy_update_ind = 0\n",
    "\n",
    "        return self.Rave, self.theta_mu, self.theta_sigma, self.w_valuefunc\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        chooses an action from a Gaussian distribution\n",
    "        \n",
    "        an action, a temperature threshold, is sampled from a Gaussian \n",
    "        distribution whose mean and variance are calculated based on the\n",
    "        theta_mu and theta_sigma and a feature state vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        # choosing action (threshold temperatures)\n",
    "#         T, hs, aT, zT = state\n",
    "#         feature_vec = Stochastic_AC_PG.feature_vec(state)\n",
    "#         mu = np.dot(self.theta_mu, feature_vec)\n",
    "        \n",
    "        mu = self.theta_mu\n",
    "        sigma = np.exp(self.theta_sigma)\n",
    "        \n",
    "        while True:\n",
    "            action = np.random.normal(mu, sigma)\n",
    "            print('l1 is:',action)\n",
    "            if action>self.normalize_action_func(self.l1low) and action<self.normalize_action_func(self.l1up):\n",
    "                break\n",
    "                \n",
    "        return action\n",
    "\n",
    "    def learn(self, S, A, Sp, R, delta_time):\n",
    "        \"\"\"\n",
    "        this function updates all the system parameters we want to learn\n",
    "        \n",
    "        this function takes the initial state (S) and the action taken \n",
    "        at S (A) as well as the next state (Sp) and the reward (R) and\n",
    "        the transition time (delta_time), and use them all to update all\n",
    "        the system parameters we want to learn (those initialized under\n",
    "        the initialization function)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.policy_update_ind = self.policy_update_ind + 1\n",
    "\n",
    "#         if self.PerTimeStep:\n",
    "#             delt = R - self.Rave * delta_time + self.value_func(\n",
    "#                 Sp) - self.value_func(S)\n",
    "#             self.Rave = self.Rave + self.eta * delt / delta_time\n",
    "#         else:\n",
    "#             delt = R - self.Rave + self.value_func(Sp) - self.value_func(S)\n",
    "#             self.Rave = self.Rave + self.eta * delt\n",
    "\n",
    "        \n",
    "        delt = R - self.Rave + self.value_func(Sp) - self.value_func(S)\n",
    "        self.Rave = self.Rave + self.eta * delt\n",
    "        \n",
    "#         delt = R + self.value_func(Sp) - self.value_func(S)\n",
    "    \n",
    "        dV_dw = self.value_func_grad(S)\n",
    "        dlnPi_dtheta_mu = self.policy_func_grad(S, A)[0]\n",
    "        dlnPi_dtheta_sigma = self.policy_func_grad(S, A)[1]\n",
    "\n",
    "        self.z_w = self.lambda_w * self.z_w + dV_dw\n",
    "        self.z_theta_mu = self.lambda_theta * self.z_theta_mu + dlnPi_dtheta_mu\n",
    "        self.z_theta_sigma = self.lambda_theta * self.z_theta_sigma +\\\n",
    "                              dlnPi_dtheta_sigma\n",
    "\n",
    "        self.w_valuefunc = self.w_valuefunc + self.alpha_w * delt * self.z_w\n",
    "        \n",
    "        if self.policy_update_ind % self.policy_update_freq == 0:\n",
    "#             self.theta = self.theta + self.alpha_theta * dmu_dtheta * dQ_da\n",
    "            self.theta_mu = self.theta_mu +\\\n",
    "                            self.alpha_theta_mu * delt * self.z_theta_mu\n",
    "            self.theta_sigma = self.theta_sigma +\\\n",
    "                               self.alpha_theta_sigma * delt * self.z_theta_sigma\n",
    "\n",
    "        #         ipdb.set_trace()\n",
    "\n",
    "        return self.Rave, self.theta_mu, self.theta_sigma, self.w_valuefunc\n",
    "\n",
    "    def value_func(self, state):\n",
    "        \"\"\"\n",
    "        this function evaluates the state-value function at a given\n",
    "        state\n",
    "        \"\"\"\n",
    "        \n",
    "#         T, hs, aT, zT = state\n",
    "#         feature_vec = Stochastic_AC_PG.feature_vec(state)\n",
    "#         np.dot(self.w_valuefunc, feature_vec)\n",
    "        return self.w_valuefunc\n",
    "\n",
    "    def value_func_grad(self, state):\n",
    "        \"\"\"\n",
    "        this function evaluates the state-value function gradient wrt\n",
    "        its parameters at a given state\n",
    "        \"\"\"\n",
    "        \n",
    "#         T, hs, aT, zT = state\n",
    "#         feature_vec = Stochastic_AC_PG.feature_vec(state)\n",
    "#         dVdw = feature_vec\n",
    "        dVdw = 1.0\n",
    "        return dVdw\n",
    "\n",
    "    def policy_func_grad(self, state, A):\n",
    "        \"\"\"\n",
    "        this function evaluates the policy (its log) gradient wrt\n",
    "        its parameters at a given state and action\n",
    "        \"\"\"\n",
    "        \n",
    "#         T, hs, aT, zT = state\n",
    "#         feature_vec = Stochastic_AC_PG.feature_vec(state)\n",
    "#         mu = np.dot(self.theta_mu, feature_vec)\n",
    "        \n",
    "        mu = self.theta_mu\n",
    "        sigma = np.exp(self.theta_sigma)\n",
    "\n",
    "        #         ipdb.set_trace()\n",
    "\n",
    "        m, s = sp.symbols('m s')\n",
    "        lnPi = sp.log(\n",
    "            (1 / (s * sp.sqrt(2 * sp.pi))) * sp.exp(-(A - m)**2 / (2 * s**2)))\n",
    "\n",
    "        dlnPi_dmu = sp.diff(lnPi, m)\n",
    "        dlnPi_dsigma = sp.diff(lnPi, s)\n",
    "\n",
    "        dlnPi_dmu_calc = sp.lambdify((m, s), dlnPi_dmu, 'numpy')\n",
    "        dlnPi_dsigma_calc = sp.lambdify((m, s), dlnPi_dsigma, 'numpy')\n",
    "\n",
    "        dlnPi_dtheta = np.array([dlnPi_dmu_calc(mu, sigma),\\\n",
    "                                 dlnPi_dsigma_calc(mu, sigma) * sigma])\n",
    "        return dlnPi_dtheta\n",
    "\n",
    "#     @classmethod\n",
    "    def normalize_action_func(self, x):\n",
    "        \"\"\" this normalizes the action\"\"\"\n",
    "        \n",
    "        normalized_x = 2*(x-self.l1low)/(self.l1up-self.l1low)-1\n",
    "        return normalized_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fec494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea712df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fenicsSBL",
   "language": "python",
   "name": "fenicssbl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
